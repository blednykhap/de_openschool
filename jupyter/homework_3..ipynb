{
  "metadata": {
    "name": "homework_3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# вывести в виде\n# +--------------------------------+----------+\n# |category_code                   |view_count|\n# +--------------------------------+----------+\n# |null                            |13236458  |\n# |electronics.smartphone          |10619448  |\n# |electronics.clocks              |1272783   |\n# |computers.notebook              |1106406   |\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import count\n\nmarket_events \u003d spark.table(\"user_id.market_events\")\naggregeted_df \u003d market_events \\\n    .groupBy(\"category_code\").agg(count(\"*\").alias(\"view_count\")) \\\n    .orderBy(col(\"view_count\").desc())\n\naggregeted_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# результат визуализировать через z.show()\n\nfrom pyspark.sql.functions import col, round, countDistinct\n\nmarket_events \u003d spark.table(\"user_id.market_events\")\n\ndaily_10 \u003d market_events \\\n    .where(col(\"event_date\") \u003d\u003d \"2019-10-10\") \\\n    .withColumn(\"bin\", round(\"price\", -1)) \\\n    .groupBy(\"bin\").agg(countDistinct(\"product_id\").alias(\"cnt\")) \\\n    .orderBy(col(\"bin\"))\n\nz.show(daily_10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# вывести через df.show() в виде\n# +--------+------+\n# |is_apple| count|\n# +--------+------+\n# |    true| *****|\n# |   false| *****|\n# +--------+------+\n\nfrom pyspark.sql.functions import col, expr, when, count\n\napple_events \u003d spark.table(\"user_id.market_events\") \\\n    .where(expr(\"event_time between \u00272019-10-01\u0027 and \u00272019-11-01\u0027\")) \\\n    .withColumn(\"is_apple\", when(col(\"brand\") \u003d\u003d \"apple\", \"true\").otherwise(\"false\")) \\\n    .withColumn(\"col_order\", when(col(\"brand\") \u003d\u003d \"apple\", 1).otherwise(2)) \\\n    .groupBy(\"col_order\", \"is_apple\").agg(count(\"*\").alias(\"count\")) \\\n    .orderBy(\"col_order\") \\\n    .select(\"is_apple\", \"count\")\n\napple_events.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# z.show(), ключ -- часы, значения -- число продаж и сумма прибыли за этот час\n\nfrom pyspark.sql.functions import date_format, count, sum, expr, round, date_trunc, lit, col\n\n# hourly_count_graph - исключительно для наглядности при построении графика\n\nhourly_sales \u003d spark.table(\"user_id.market_events\") \\\n    .where(expr(\"event_time between \u00272019-10-07\u0027 and \u00272019-10-14\u0027\")) \\\n    .withColumn(\"hour_id\", date_trunc(\"hour\", \"event_time\")) \\\n    .groupBy(\"hour_id\").agg(count(\"*\").alias(\"hourly_count\"), sum(\"price\").alias(\"hourly_sum\")) \\\n    .select(\"hour_id\", \"hourly_count\", \"hourly_sum\") \\\n    .withColumn(\"hourly_count_graph\", col(\"hourly_count\") * lit(100)) \\\n    .orderBy(\"hour_id\")\n\nz.show(hourly_sales)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# z.show(), ключ -- час в диапазоне от 0 до 23, значение -- усредненное за месяц число продаж в этот час на месячных данных\n\nfrom pyspark.sql.functions import expr, hour, count, date_trunc, avg, sum, col, lit\n\n# avg_count_daily_graph - исключительно для наглядности при построении графика\n\nhourly_sales \u003d spark.table(\"user_id.market_events\") \\\n    .where(expr(\"event_time between \u00272019-10-01\u0027 and \u00272019-11-01\u0027\")) \\\n    .withColumn(\"hour_monthly\", date_trunc(\"hour\", \"event_time\")) \\\n    .groupBy(\"hour_monthly\").agg(count(\"*\").alias(\"count_monthly\"), sum(\"price\").alias(\"sum_monthly\")) \\\n    .withColumn(\"hour_daily\", hour(\"hour_monthly\")) \\\n    .groupBy(\"hour_daily\").agg(avg(\"count_monthly\").alias(\"avg_count_daily\"), avg(\"sum_monthly\").alias(\"avg_sum_t_daily\")) \\\n    .withColumn(\"avg_count_daily_graph\", col(\"avg_count_daily\") * lit(100)) \\\n    .orderBy(\"hour_daily\")\n\nz.show(hourly_sales)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}