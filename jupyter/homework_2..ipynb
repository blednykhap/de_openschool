{
  "metadata": {
    "name": "homework_2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Задание после лекции 2.\n\nСохранить один месяц данных из папки /datasets/marketplace в личную базу username.market_events, данные партиционировать по дате\n\nПосчитать посуточные аггрегаты для этих данных по категориям в таблицу event_types_daily с колонками: event_date, category_id, event_type, event_count, distinct_customer_count. Счёт нужно производить отдельно по дням (в цикле по датам), целевую таблицу тоже разделить на партиции.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Инициализация"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Создаем базу данных\nspark.sql(\"create database if not exists user_id\")\ndf \u003d spark.sql(\"show databases\")\ndf.show()\n\n#spark.sql(\"CREATE TABLE user_id.event_types_daily (category_id int, event_type string, event_count int, distinct_customer_count int, event_date timestamp) USING parquet PARTITIONED BY (event_date)\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Вариант 1. Решение агрегацией "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import col, to_timestamp, to_date\nfrom pyspark.sql.functions import count\nfrom pyspark.sql.functions import countDistinct\n\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\nprint(\"Start loading\")\n\n# Чтение данных из файла (hadcode)\nmarked_df \u003d spark.read.csv(\"/datasets/marketplace/2019-Nov.csv.gz\", header\u003dTrue, inferSchema\u003dTrue)\n\n# Трансформация поля партиционирования\ntransformed_df \u003d marked_df \\\n    .withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn(\"event_date\", to_date(col(\"event_time\")))\n\n# Формирование таблицы источника\ntransformed_df \\\n    .write \\\n    .partitionBy(\"event_date\") \\\n    .mode(\"overwrite\") \\\n    .saveAsTable(\"user_id.market_events\")\n    \n# Получение целевых данных плюс агрегация\nmarked_df \u003d spark.table(\"user_id.market_events\")\naggregated_df \u003d marked_df \\\n    .groupBy(\"event_date\", \"category_id\", \"event_type\") \\\n    .agg(count(\"*\").alias(\"event_count\"), countDistinct(\"user_id\").alias(\"distinct_customer_count\"))    \n    \n# Запись целевых данных    \naggregated_df \\\n    .repartition(\"event_date\") \\ \n    .write \\\n    .partitionBy(\"event_date\") \\\n    .mode(\"overwrite\") \\\n    .saveAsTable(\"user_id.event_types_daily\")    \n    \nprint(\"Loading complite\")    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Вариант 2. Итерация по дням\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import col, date_format, count, countDistinct\n\nprint(\"Start loading\")\n\n# Получение даты событий из источника\nmarket_events_df \u003d spark.table(\"user_id.market_events\")\nmarket_events_dates \u003d market_events_df \\\n    .select(\"event_date\") \\\n    .distinct()\n\n# Получение даты событий из целевой таблицы\nevent_types_daily_df \u003d spark.table(\"user_id.event_types_daily\")\nevent_types_daily_dates \u003d event_types_daily_df \\\n    .select(\"event_date\") \\\n    .distinct() \n    \n# Формирование список дат для загрузки    \netl_dates \u003d market_events_dates \\\n    .subtract(event_types_daily_dates) \\\n    .orderBy(\"event_date\") \\\n    .withColumn(\"event_date\", date_format(\"event_date\", \"yyyy-MM-dd\")) \\\n    .rdd.map(lambda x: x[0]).collect()\n\n# Итерация по дням загрузки\nfor current_date in etl_dates:\n    \n    # Получение агрегатов за 1 день\n    marked_df \u003d spark.table(\"user_id.market_events\")\n    aggregated_df \u003d marked_df \\\n        .where(col(\"event_date\") \u003d\u003d current_date) \\\n        .groupBy(\"event_date\", \"category_id\", \"event_type\") \\\n        .agg(count(\"*\").alias(\"event_count\"), countDistinct(\"user_id\").alias(\"distinct_customer_count\"))\n        \n    # Сохранение данных\n    aggregated_df \\\n        .repartition(\"event_date\") \\\n        .write \\\n        .partitionBy(\"event_date\") \\\n        .mode(\"append\") \\\n        .saveAsTable(\"user_id.event_types_daily\")       \n        #.mode(\"overwrite\") \\ - Предпочтительно, при корректно работающей опции spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \n\n    print(f\"Load data for {current_date} complite\")      \n\nprint(\"Loading complite\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nspark.stop()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Проверки"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import col\n\nspark.sql(\"ALTER TABLE user_id.event_types_daily DROP IF EXISTS PARTITION (event_date\u003d\u00272019-11-29\u0027)\")\n\nevent_types_daily_df \u003d spark.table(\"user_id.event_types_daily\")\nresult_df \u003d event_types_daily_df.orderBy(col(\u0027event_date\u0027).desc())\n\nz.show(result_df)\n"
    }
  ]
}