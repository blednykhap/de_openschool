{
  "metadata": {
    "name": "homework_1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Задание после вводной лекции\n\n## Редактировать эту тетрадку или запускать нельзя, нужно сперва скопировать её по пути /username/homework_1\n\n\n## Задание первое -- запустить спарк приложение и вывести его настройки с помощью print()\n\nПервый шаг -- запустить параграф \"Задание 1\" с интерпретером spark. При этом запустится zeppelin interpreter, который запустит Spark Application на Hadoop Yarn. Запущенное спарк приложение отобразится в списке здесь [Yarn Application List](http://ca-spark-n-01.innoca.local:8088/ui2/#/yarn-apps/apps). Пока интерпретер работает, вам доступна SparkSession в переменной spark. Из неё можно, например, получить конфигурацию спарк приложения (задание 1)\n\nЕсли нажать на странице Yarn Application List на ваш application_id, откроется его страница в Yarn. На ней нужно найти ссылку Application Master, пройти по ней в Spark UI. Это интерфейс приложения Spark, который доступен только пока приложение работает. \n\n\n## Важно! На каждую тетрадку запускается новый интерпретер. По завершении работы обязательно останавливать спарк приложение вызовом spark.stop() для освобождения ресурсов кластера\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# нужно отфильтровать конфиги по маске spark.* и распечатать с помощью print()\n# должно получиться что-то вроде такого:\n# spark.shuffle.service.enabled: False\n\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\nlist_with_configs \u003d spark.sparkContext.getConf().getAll()\nlist_with_configs"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.stop()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfor k, v in list_with_configs:\n    if k.find(\u0027spark.\u0027) \u003d\u003d 0:\n        print(f\u0027{k}: {v}\u0027)\n        "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}