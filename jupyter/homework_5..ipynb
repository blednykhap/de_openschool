{
  "metadata": {
    "name": "homework_5",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# RDD"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd \u003d sc.parallelize([(1,2), (3,4), (3,6), (4,5), (3, 4), (1, 5), (4, 1)])\n\nresult_rdd \u003d rdd \\\n    .reduceByKey(lambda x, y: (x + y))\n\nresult_rdd.take(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nlines \u003d sc.parallelize([\n    \"a ab abc\",\n    \"a ac abc\",\n    \"b b ab abc\"\n    ])\n\ncounts \u003d lines.flatMap(lambda x: x.split(\u0027 \u0027)) \\\n    .map(lambda x: (x, 1)) \\\n    .reduceByKey(lambda x, y: (x + y))\n   \n# дополнить код, чтобы получился rdd из пар (слово, частота)\n\noutput \u003d counts.collect()\n\nfor (word, count) in output:\n    print(\"%s: %i\" % (word, count))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# market.events"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import regexp_extract\n\nevents_df \u003d spark.table(\"market.events\")\n\nevents_cat_df \u003d events_df \\\n    .withColumn(\"cat_1\", regexp_extract(\"category_code\", \"([a-z_]*)?.?([a-z_]*)?.?([a-z_]*)?\", 1) ) \\\n    .withColumn(\"cat_2\", regexp_extract(\"category_code\", \"([a-z_]*)?.?([a-z_]*)?.?([a-z_]*)?\", 2) ) \\\n    .withColumn(\"cat_3\", regexp_extract(\"category_code\", \"([a-z_]*)?.?([a-z_]*)?.?([a-z_]*)?\", 3) ) \\\n\nevents_cat_df.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col, count, row_number\nfrom pyspark.sql.window import Window\n\nwindowSpec \u003d Window.partitionBy(\"cat_2\").orderBy(col(\"views\").desc())\n\nevents_rank_df \u003d events_cat_df \\\n    .where(col(\"brand\") !\u003d \u0027null\u0027) \\\n    .groupBy(\"cat_1\", \"cat_2\", \"brand\").agg(count(\"*\").alias(\"views\")) \\\n    .withColumn(\"rank\", row_number().over(windowSpec)) \\\n    .where(col(\"rank\") \u003c\u003d 3) \\\n    .orderBy(\"cat_2\", col(\"views\").desc())\n    \nevents_rank_df.show()    \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Датасет с треками\n### !!! Внесены небольшие изменения, в частности, сохранена промежуточная таблица (без explode)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import *\n\nsch\u003dArrayType(StringType());\n\n# важно что разделитель \u0027, \u0027 с пробелом, иначе пробелы добавятся в значения\ntracks \u003d spark.read.option(\"header\", \"true\") \\\n        .option(\"escape\", \u0027\"\u0027) \\\n        .option(\"InferSchema\", \"true\") \\\n        .csv(\"/datasets/tracks.csv\") \\\n        .withColumn(\"release_year\", f.substring(\"release_date\", 1, 4).cast(IntegerType())) \\\n        .withColumn(\"array_artist\", f.split(f.regexp_replace(f.col(\"artists\"), \"[\\]\\[\\\u0027]\", \"\"),\", \")) \\\n        .cache() #выделяем год в отдельную колонку и преобразуем колонку с артистами в массив\n\ntracks.write.mode(\"overwrite\").saveAsTable(\"user_id.tracks\")\n\ntracks_exp \u003d tracks.select(  \n                            \"name\", \n                            \"popularity\",\n                            \"danceability\",\n                            \"energy\",\n                            \"speechiness\",\n                            \"acousticness\",\n                            \"liveness\",\n                            \"valence\",\n                            \"release_year\",\n                            \"artists\",\n                            f.explode(f.col(\"array_artist\") ).alias(\"name_artist\")\n                        ) #создаем отдельную таблицу с развернутым массивом артистов\n                        \ntracks_exp.printSchema()\n\ntracks_exp.write.mode(\"overwrite\").saveAsTable(\"user_id.tracks_exp\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntracks \u003d spark.table(\"hw_3.tracks\")\nz.show(tracks)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nwindow \u003d Window.orderBy(F.col(\u0027popularity\u0027).desc()).partitionBy(\u0027release_year\u0027)\n\ntop_100 \u003d (tracks\n        .where(F.col(\u0027popularity\u0027) \u003e 0)\n        .withColumn(\u0027rank\u0027, F.rank().over(window))\n        .where(F.col(\u0027rank\u0027) \u003c 101)\n        )\n\ntracks_agg \u003d (top_100\n            .groupBy(\u0027name_artist\u0027)\n            .agg(F.count(\u0027*\u0027).alias(\u0027count\u0027))\n            .orderBy(F.col(\u0027count\u0027).desc())\n    )\n\ntracks_agg.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col, row_number, explode\nfrom pyspark.sql.window import Window\n\nwindowSpec \u003d Window.partitionBy(\"release_year\").orderBy(col(\"popularity\").desc(), \"name\")\n\ntracks \u003d spark.table(\"user_id.tracks\")\n\n#.select(\"name\", \"popularity\", \"release_year\", \"array_artist\") \\\ntop_100_tracks \u003d tracks \\\n    .dropDuplicates() \\\n    .withColumn(\"rn\", row_number().over(windowSpec)) \\\n    .where(col(\"rn\") \u003c\u003d 100) \\\n    .orderBy(\"release_year\", \"rn\") \n    \ntop_100_tracks_exp \u003d top_100_tracks \\\n    .select(\"release_year\", \"array_artist\", explode(col(\"array_artist\")).alias(\"name_artist\")) \\\n    .cache()\n    \ntop_100_tracks_exp.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col\n\ntop_100_tracks_by_artists_by_countity \u003d top_100_tracks_exp \\\n    .groupBy(\"name_artist\").count() \\\n    .orderBy(col(\"count\").desc())\n    \ntop_100_tracks_by_artists_by_countity.show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n   \nfrom pyspark.sql.functions import col \n\ntop_100_tracks_by_artists_by_rate \u003d top_100_tracks_exp \\\n    .select(\"name_artist\", \"release_year\").distinct() \\\n    .groupBy(\"name_artist\").count() \\\n    .orderBy(col(\"count\").desc())    \n    \ntop_100_tracks_by_artists_by_rate.show()      \n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# учтем тот факт, что если пронумеровать отсортированные года, которые сгруппированны по исполнителю, \n# а затем от года отнять соответствующий номер, то разница между ними, для подряд идущих годов, даст ровно одну группу\n\nfrom pyspark.sql.functions import col, row_number, count, max\nfrom pyspark.sql.window import Window\n\nwindowRn \u003d Window.partitionBy(\"name_artist\").orderBy(\"release_year\")  \n    \ntop_100_tracks_by_artists_by_year_1 \u003d top_100_tracks_exp \\\n    .select(\"name_artist\", \"release_year\").distinct() \\\n    .withColumn(\"rn\", row_number().over(windowRn)) \\\n    .withColumn(\"dif\", col(\"release_year\") - col(\"rn\")) \\\n    .groupBy(\"name_artist\", \"dif\").agg(count(\"*\").alias(\"cnt\")) \\\n    .groupBy(\"name_artist\").agg(max(\"cnt\").alias(\"mx\")) \\\n    .orderBy(col(\"mx\").desc()) \n\ntop_100_tracks_by_artists_by_year_1.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# суть решения в том, чтобы передать года выпуска треков по одному артисту в виде списка\n# далее отсортировать список, пройтисть по каждому элементу, и если он отличася от предудущего более чем на 1, добавить разделитель\n# затем разбить список на несколько по разделителю и определить, какой их новых списков максимального размера\n\nfrom pyspark.sql.functions import  col, collect_list, udf\nfrom pyspark.sql.types import IntegerType\n\ndef getChainLen(years):\n\n    years.sort()\n    sep_years \u003d []\n    \n    for x in years:\n        if len(sep_years) !\u003d 0 and (int(sep_years[-1]) + 1) !\u003d x:\n            sep_years.append(str(\u0027s\u0027))\n            sep_years.append(str(x))\n        else:\n            sep_years.append(str(x))\n\n    sep_years \u003d \" \".join(sep_years).split(\u0027s\u0027) \n\n    y \u003d 0\n    for x in sep_years:\n        x \u003d x.strip().split()\n        y \u003d len(x) if len(x) \u003e y else y\n    \n    return y\n    \ngetChainLenUDF \u003d udf(lambda z:getChainLen(z), IntegerType())    \n    \ntop_100_tracks_by_artists_by_year_2 \u003d top_100_tracks_exp \\\n    .select(\"name_artist\", \"release_year\").distinct() \\\n    .groupBy(\"name_artist\").agg(collect_list(\"release_year\").alias(\"years\")) \\\n    .withColumn(\"cnt\", getChainLenUDF(col(\"years\"))) \\\n    .select(\"name_artist\", \"cnt\") \\\n    .orderBy(col(\"cnt\").desc(), \"name_artist\")\n    \ntop_100_tracks_by_artists_by_year_2.show()    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# https://stackoverflow.com/questions/56384625/pyspark-cumulative-sum-with-reset-condition\n\nfrom pyspark.sql.functions import col, lag, lit, sum, max, when\nfrom pyspark.sql.window import Window\n\nwindowLag \u003d Window.partitionBy(\"name_artist\").orderBy(col(\"release_year\"))\nwindowGrp \u003d Window.orderBy(\"name_artist\", \"release_year\")\n\ntop_100_tracks_by_artists_by_year_3 \u003d top_100_tracks_exp \\\n    .select(\"name_artist\", \"release_year\").distinct() \\\n    .withColumn(\"lag\", lag(\"release_year\").over(windowLag)) \\\n    .withColumn(\"dif\", col(\"release_year\") - col(\"lag\")) \\\n    .withColumn(\"tag\", when(col(\"dif\") !\u003d 1, lit(None)).otherwise(col(\"dif\"))) \\\n    .withColumn(\"grp\", sum((col(\"tag\").isNull()).cast(\"int\")).over(windowGrp)) \\\n    .groupBy(\"name_artist\", \"grp\").count() \\\n    .groupBy(\"name_artist\").agg(max(col(\"count\")).alias(\"cnt\")) \\\n    .orderBy(col(\"cnt\").desc(), \"name_artist\")\n\ntop_100_tracks_by_artists_by_year_3.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import expr, col, avg, round\n\ntracks \u003d spark.table(\"user_id.tracks_exp\").dropDuplicates()\n    \n##########################################################\n\nyear_means \u003d tracks \\\n    .select(\"release_year\", \"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"liveness\", \"valence\") \\\n    .groupBy(\"release_year\") \\\n    .agg( \\\n        round(avg(\"danceability\"), 2).alias(\"danceability_mean\"), \\\n        round(avg(\"energy\"), 2).alias(\"energy_mean\"), \\\n        round(avg(\"speechiness\"), 2).alias(\"speechiness_mean\"), \\\n        round(avg(\"acousticness\"), 2).alias(\"acousticness_mean\"), \\\n        round(avg(\"liveness\"), 2).alias(\"liveness_mean\"), \\\n        round(avg(\"valence\"), 2).alias(\"valence_mean\"))\n   \nyear_means.show()\n\n##########################################################\n\ntracks_advanced \u003d tracks.join(year_means, \"release_year\") \\\n    .withColumn(\"norm_danceability_mean\", round(col(\"danceability\")/col(\"danceability_mean\"), 2)) \\\n    .withColumn(\"norm_energy_mean\", round(col(\"energy\")/col(\"energy_mean\"), 2)) \\\n    .withColumn(\"norm_speechiness_mean\", round(col(\"speechiness\")/col(\"speechiness_mean\"), 2)) \\\n    .withColumn(\"norm_acousticness_mean\", round(col(\"acousticness\")/col(\"acousticness_mean\"), 2)) \\\n    .withColumn(\"norm_liveness_mean\", round(col(\"liveness\")/col(\"liveness_mean\"), 2)) \\\n    .withColumn(\"norm_valence_mean\", round(col(\"valence\")/col(\"valence_mean\"), 2))\n\ntracks_advanced.show()\n\n##########################################################\n\nunpivot_expr \u003d \"stack(6, \u0027norm_danceability_mean\u0027, norm_danceability_mean, \u0027norm_energ_meany\u0027, norm_energy_mean, \u0027norm_speechiness_mean\u0027, norm_speechiness_mean, \\\n    \u0027norm_acousticness_mean\u0027, norm_acousticness_mean, \u0027norm_liveness_mean\u0027, norm_liveness_mean, \u0027norm_valence_mean\u0027, norm_valence_mean) as (characteristic, val)\"\n\ncharacteristics \u003d [\"norm_danceability_mean\", \"norm_energ_meany\", \"norm_speechiness_mean\", \"norm_acousticness_mean\", \"norm_liveness_mean\", \"norm_valence_mean\"]\n\ntracks_advanced_unpivot \u003d tracks_advanced \\\n    .select(\"name_artist\", expr(unpivot_expr))    \n    \nfor item in characteristics:\n    \n    top_by_characteristic \u003d tracks_advanced_unpivot \\\n        .where(col(\"characteristic\") \u003d\u003d item) \\\n        .groupBy(\"name_artist\").agg(avg(\"val\").alias(item)) \\\n        .orderBy(col(item).desc()) \n        \n    top_by_characteristic.show(3)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}