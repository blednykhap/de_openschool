{
  "paragraphs": [
    {
      "text": "%spark.conf\n\nspark.executor.instances=2\nspark.executor.memory=1G\nspark.kryoserializer.buffer.max=1024m\n\nspark.sql.autoBroadcastJoinThreshold=20971520",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:25:45+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_625245315",
      "id": "20211125-171509_574539172",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:25:45+0300",
      "dateFinished": "2024-02-11T18:25:45+0300",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:21827"
    },
    {
      "title": "Как выполнять",
      "text": "%md\n\nНужно скопировать себе эту тетрадку. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:25:51+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Нужно скопировать себе эту тетрадку. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1502785914",
      "id": "20201127-213054_1829929461",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:25:51+0300",
      "dateFinished": "2024-02-11T18:25:51+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21828"
    },
    {
      "title": "Генерация events таблицы (код для ознакомления, запускать не нужно)",
      "text": "import org.apache.spark.mllib.random.RandomRDDs._\nimport java.time.LocalDate\nimport java.time.format.DateTimeFormatter\n\nval dates = (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n\ndef generateCity(r: Double): String = if (r < 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n\ndef generateCityUdf = udf(generateCity _)\n\n// spark.sql(\"drop table hw2.events_full\")\nspark.sql(\"create database hw_4\")\nfor(i <- dates) {\n    uniformRDD(sc, 10000000L, 1)\n    .toDF(\"uid\")\n    .withColumn(\"date\", lit(i))\n    .withColumn(\"city\", generateCityUdf($\"uid\"))\n    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n    .withColumn(\"skew_key\", when($\"city\" === \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n    .write.mode(\"append\")\n    .partitionBy(\"date\")\n    .saveAsTable(\"hw_4.events_full\")\n}\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-09T13:01:02+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.mllib.random.RandomRDDs._\nimport java.time.LocalDate\nimport java.time.format.DateTimeFormatter\n\u001b[1m\u001b[34mdates\u001b[0m: \u001b[1m\u001b[32mscala.collection.immutable.Seq[String]\u001b[0m = Vector(2020-11-01, 2020-11-02, 2020-11-03, 2020-11-04, 2020-11-05, 2020-11-06, 2020-11-07, 2020-11-08, 2020-11-09, 2020-11-10, 2020-11-11, 2020-11-12, 2020-11-13, 2020-11-14, 2020-11-15)\n\u001b[1m\u001b[34mgenerateCity\u001b[0m: \u001b[1m\u001b[32m(r: Double)String\u001b[0m\n\u001b[1m\u001b[34mgenerateCityUdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1744508650",
      "id": "20201127-224038_803369215",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "status": "READY",
      "$$hashKey": "object:21829"
    },
    {
      "title": "Генерация events_sample",
      "text": "spark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.001)\n.repartition(2)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample\")\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-09T13:01:02+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1460329877",
      "id": "20201127-230139_1962818180",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "status": "READY",
      "$$hashKey": "object:21830"
    },
    {
      "text": "\nspark.table(\"hw_4.sample\")\n.limit(100)\n.coalesce(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_small\")",
      "user": "anonymous",
      "dateUpdated": "2024-02-09T13:01:02+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_752910006",
      "id": "20201128-000812_530567540",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "status": "READY",
      "$$hashKey": "object:21831"
    },
    {
      "text": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.003)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_big\")",
      "user": "anonymous",
      "dateUpdated": "2024-02-09T13:01:02+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_784408415",
      "id": "20201128-091248_492627774",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "status": "READY",
      "$$hashKey": "object:21832"
    },
    {
      "text": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.015)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_very_big\")",
      "user": "anonymous",
      "dateUpdated": "2024-02-09T13:01:02+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1318944857",
      "id": "20201128-093907_1614062530",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "status": "READY",
      "$$hashKey": "object:21833"
    },
    {
      "title": "Задание 1",
      "text": "%md\n\n\n\nДля упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n\nОтветить на вопросы:\n * какова структура таблиц\n * сколько в них записей \n * сколько места занимают данные\n ",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:26:05+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Для упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big].</p>\n<p>Ответить на вопросы:</p>\n<ul>\n<li>какова структура таблиц</li>\n<li>сколько в них записей</li>\n<li>сколько места занимают данные</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_2029911926",
      "id": "20201128-094640_2955666",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:26:05+0300",
      "dateFinished": "2024-02-11T18:26:05+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21834"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import col\n\ntables = spark.sql(\"SHOW TABLES IN hw_4\").collect()\n\nfor table in tables:\n    \n    # Имя объекта\n    print(f\"{table['database']}.{table['tableName']}\")\n    \n    # Статистика\n    compute_df = spark.sql(f\"ANALYZE TABLE {table['database']}.{table['tableName']} COMPUTE STATISTICS\")\n    describe_df = spark.sql(f\"DESCRIBE FORMATTED {table['database']}.{table['tableName']}\")\n    statistics = describe_df \\\n        .where(col(\"col_name\") == \"Statistics\") \\\n        .select(\"data_type\") \\\n        .rdd.map(lambda x: x[0]) \\\n        .collect()\n    print(statistics[0])    \n    \n    # Схема таблицы\n    spark.table(f\"{table['database']}.{table['tableName']}\").printSchema()\n    ",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:26:16+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "hw_4.events_full\n10695245573 bytes, 150000000 rows\nroot\n |-- event_id: string (nullable = true)\n |-- city: string (nullable = true)\n |-- skew_key: string (nullable = true)\n |-- date: string (nullable = true)\n\nhw_4.sample\n9580949 bytes, 150397 rows\nroot\n |-- event_id: string (nullable = true)\n\nhw_4.sample_big\n28608657 bytes, 449135 rows\nroot\n |-- event_id: string (nullable = true)\n\nhw_4.sample_small\n7214 bytes, 100 rows\nroot\n |-- event_id: string (nullable = true)\n\nhw_4.sample_very_big\n143346266 bytes, 2250691 rows\nroot\n |-- event_id: string (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=0",
              "$$hashKey": "object:23267"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=1",
              "$$hashKey": "object:23268"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=2",
              "$$hashKey": "object:23269"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=3",
              "$$hashKey": "object:23270"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=4",
              "$$hashKey": "object:23271"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=5",
              "$$hashKey": "object:23272"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=6",
              "$$hashKey": "object:23273"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=7",
              "$$hashKey": "object:23274"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=8",
              "$$hashKey": "object:23275"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=9",
              "$$hashKey": "object:23276"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1837042443",
      "id": "20211126-201009_83206942",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:26:16+0300",
      "dateFinished": "2024-02-11T18:27:05+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21835"
    },
    {
      "text": "%md\n\nИнформация о том, сколько места занимают данные посмотреть в HDFS UI",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:27:14+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Информация о том, сколько места занимают данные посмотреть в HDFS UI</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1658892073",
      "id": "20211125-203343_1693054050",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:27:14+0300",
      "dateFinished": "2024-02-11T18:27:14+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21836"
    },
    {
      "title": "Задание 2",
      "text": "%md\n\nПолучить планы запросов для джойна большой таблицы hw_4.events_full с каждой из таблиц hw_4.sample, hw_4.sample_big, hw_4.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? \n\nBroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:27:21+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Получить планы запросов для джойна большой таблицы hw_4.events_full с каждой из таблиц hw_4.sample, hw_4.sample_big, hw_4.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin?</p>\n<p>BroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(&ldquo;spark.sql.autoBroadcastJoinThreshold&rdquo;).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1563233137",
      "id": "20201128-132950_831220047",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:27:21+0300",
      "dateFinished": "2024-02-11T18:27:21+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21837"
    },
    {
      "text": "%pyspark\n\nconf = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nprint(conf)\n\nevents_full_df = spark.table(\"hw_4.events_full\")\n\n# BroadcastHashJoin\nsample_small_df = spark.table(\"hw_4.sample_small\")\nef_sample_small_df = events_full_df \\\n    .join(sample_small_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# BroadcastHashJoin\nsample_df = spark.table(\"hw_4.sample\")\nef_sample_df = events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# SortMergeJoin\nsample_big_df = spark.table(\"hw_4.sample_big\")\nef_sample_big_df = events_full_df \\\n    .join(sample_big_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# SortMergeJoin\nsample_very_big_df = spark.table(\"hw_4.sample_very_big\")\nef_sample_very_big_df = events_full_df \\\n    .join(sample_very_big_df, \"event_id\", \"inner\") \\\n    .explain()\n\n#ef_sample_df.show()\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:27:40+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 331.4,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "20971520\n== Physical Plan ==\n*(2) Project [event_id#6, city#7, skew_key#8, date#9]\n+- *(2) BroadcastHashJoin [event_id#6], [event_id#69], Inner, BuildRight\n   :- *(2) Project [event_id#6, city#7, skew_key#8, date#9]\n   :  +- *(2) Filter isnotnull(event_id#6)\n   :     +- *(2) FileScan parquet hw_4.events_full[event_id#6,city#7,skew_key#8,date#9] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 15, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n      +- *(1) Project [event_id#69]\n         +- *(1) Filter isnotnull(event_id#69)\n            +- *(1) FileScan parquet hw_4.sample_small[event_id#69] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample_small], PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n== Physical Plan ==\n*(2) Project [event_id#6, city#7, skew_key#8, date#9]\n+- *(2) BroadcastHashJoin [event_id#6], [event_id#35], Inner, BuildRight\n   :- *(2) Project [event_id#6, city#7, skew_key#8, date#9]\n   :  +- *(2) Filter isnotnull(event_id#6)\n   :     +- *(2) FileScan parquet hw_4.events_full[event_id#6,city#7,skew_key#8,date#9] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 15, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n      +- *(1) Project [event_id#35]\n         +- *(1) Filter isnotnull(event_id#35)\n            +- *(1) FileScan parquet hw_4.sample[event_id#35] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample], PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n== Physical Plan ==\n*(5) Project [event_id#6, city#7, skew_key#8, date#9]\n+- *(5) SortMergeJoin [event_id#6], [event_id#52], Inner\n   :- *(2) Sort [event_id#6 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(event_id#6, 200)\n   :     +- *(1) Project [event_id#6, city#7, skew_key#8, date#9]\n   :        +- *(1) Filter isnotnull(event_id#6)\n   :           +- *(1) FileScan parquet hw_4.events_full[event_id#6,city#7,skew_key#8,date#9] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 15, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n   +- *(4) Sort [event_id#52 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(event_id#52, 200)\n         +- *(3) Project [event_id#52]\n            +- *(3) Filter isnotnull(event_id#52)\n               +- *(3) FileScan parquet hw_4.sample_big[event_id#52] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample_big], PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n== Physical Plan ==\n*(5) Project [event_id#6, city#7, skew_key#8, date#9]\n+- *(5) SortMergeJoin [event_id#6], [event_id#86], Inner\n   :- *(2) Sort [event_id#6 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(event_id#6, 200)\n   :     +- *(1) Project [event_id#6, city#7, skew_key#8, date#9]\n   :        +- *(1) Filter isnotnull(event_id#6)\n   :           +- *(1) FileScan parquet hw_4.events_full[event_id#6,city#7,skew_key#8,date#9] Batched: true, Format: Parquet, Location: CatalogFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 15, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n   +- *(4) Sort [event_id#86 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(event_id#86, 200)\n         +- *(3) Project [event_id#86]\n            +- *(3) Filter isnotnull(event_id#86)\n               +- *(3) FileScan parquet hw_4.sample_very_big[event_id#86] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample_very_..., PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1855630616",
      "id": "20211126-234713_995322419",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:27:40+0300",
      "dateFinished": "2024-02-11T18:27:41+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21838"
    },
    {
      "title": "Задание 3",
      "text": "%md\nВыполнить джойны с таблицами  hw_4.sample,  hw_4.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении\n\nЗайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  ",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:27:50+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Выполнить джойны с таблицами  hw_4.sample,  hw_4.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении</p>\n<p>Зайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1954471116",
      "id": "20201128-140231_1065047171",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:27:50+0300",
      "dateFinished": "2024-02-11T18:27:50+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21839"
    },
    {
      "text": "%pyspark\n\n# 84 tasks, \n# время выполнения 1 мин 8 сек, \n# количество task меньше, чем в операции ниже за счет наличия broadcastб \n# в даге 2 стейджа, реализующие механизм broadcast - передача малого sample датафрема на worker'ы с их последющим джойном к \"большой\" таблице event_full\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample\")\n\nevents_full_df = spark.table(\"hw_4.events_full\")\nsample_df = spark.table(\"hw_4.sample\")\n\nef_sample_df = events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_df)",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T22:16:16+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "150397\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-04.innoca.local:41098/jobs/job?id=0",
              "$$hashKey": "object:23560"
            },
            {
              "jobUrl": "http://ca-spark-d-04.innoca.local:41098/jobs/job?id=1",
              "$$hashKey": "object:23561"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_850121489",
      "id": "20211127-002540_1338416039",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T22:06:00+0300",
      "dateFinished": "2024-02-11T22:07:08+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21840"
    },
    {
      "text": "%pyspark\n\n# 284 tasks, время выполнения 2 мин 38 сек,\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample_big\")\n\nevents_full_df = spark.table(\"hw_4.events_full\")\nsample_big_df = spark.table(\"hw_4.sample_big\")\n\nef_sample_big_df = events_full_df \\\n    .join(sample_big_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T22:10:41+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "449135\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-04.innoca.local:41098/jobs/job?id=2",
              "$$hashKey": "object:23623"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_483836696",
      "id": "20211127-005457_209044171",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T22:06:34+0300",
      "dateFinished": "2024-02-11T22:09:12+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21841"
    },
    {
      "title": "Насильный broadcast",
      "text": "%md\n\nОптимизировать джойн с таблицами hw_4.sample_big, hw_4.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится ",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:30:18+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Оптимизировать джойн с таблицами hw_4.sample_big, hw_4.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1507453536",
      "id": "20201128-140749_375295552",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:30:18+0300",
      "dateFinished": "2024-02-11T18:30:18+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21842"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import broadcast\n\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample_big_broad\")\n\nevents_full_df = spark.table(\"hw_4.events_full\")\nsample_big_df = spark.table(\"hw_4.sample_big\")\nsample_big_df_broad = broadcast(sample_big_df)\n\nef_sample_big_df = events_full_df \\\n    .join(sample_big_df_broad, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:30:29+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "449135\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=13",
              "$$hashKey": "object:23727"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=14",
              "$$hashKey": "object:23728"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_494425074",
      "id": "20201224-172528_1623196266",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:30:29+0300",
      "dateFinished": "2024-02-11T18:30:57+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21843"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import broadcast\n\nsc.setLocalProperty(\"callSite.short\", \"events_full br join sample_big\")\n\nevents_full_df = spark.table(\"hw_4.events_full\")\nsample_very_big_df = spark.table(\"hw_4.sample_very_big\")\nsample_very_big_df_broad = broadcast(sample_very_big_df)\n\nef_sample_big_df = events_full_df \\\n    .join(sample_very_big_df_broad, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)\n\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:31:09+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Python process is abnormally exited, please check your code and log."
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=15",
              "$$hashKey": "object:23790"
            },
            {
              "jobUrl": "http://ca-spark-d-01.innoca.local:41710/jobs/job?id=16",
              "$$hashKey": "object:23791"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_2095998023",
      "id": "20211127-183107_1976882913",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:31:09+0300",
      "dateFinished": "2024-02-11T18:31:39+0300",
      "status": "ERROR",
      "$$hashKey": "object:21844"
    },
    {
      "text": "%md\nТаблица hw_4.sample_very_big оказывается слишком большой для бродкаста и не помещается полностью на каждой ноде, поэтому возникает исключение.\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:32:06+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Таблица hw_4.sample_very_big оказывается слишком большой для бродкаста и не помещается полностью на каждой ноде, поэтому возникает исключение.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862617_1833300802",
      "id": "20211127-184034_732892795",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:32:06+0300",
      "dateFinished": "2024-02-11T18:32:06+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21845"
    },
    {
      "title": "Отключение auto broadcast",
      "text": "%md\nОтключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса.",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:32:17+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Отключить автоматический броадкаст командой spark.conf.set(&ldquo;spark.sql.autoBroadcastJoinThreshold&rdquo;, &ldquo;-1&rdquo;). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_802134995",
      "id": "20201128-092252_410955057",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:32:17+0300",
      "dateFinished": "2024-02-11T18:32:17+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21846"
    },
    {
      "text": "%pyspark\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:32:25+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_258105923",
      "id": "20211127-024445_2082074189",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:32:25+0300",
      "dateFinished": "2024-02-11T18:32:45+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21847"
    },
    {
      "text": "%pyspark\n\n\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample without broadcast\")\n\nevents_full_df = spark.table(\"hw_4.events_full\")\nsample_df = spark.table(\"hw_4.sample\")\n\nef_sample_df = events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_df)\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:32:47+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "150397\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=0",
              "$$hashKey": "object:23987"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1642997134",
      "id": "20211127-022628_1677724626",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:32:47+0300",
      "dateFinished": "2024-02-11T18:35:05+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21848"
    },
    {
      "text": "%md\n6 минут 2 секунды в случае, когда отключен автоматический броадкастинг, по сравнению с 2 минутами 1 секундой со включенным бродкастингом.\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:35:13+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>6 минут 2 секунды в случае, когда отключен автоматический броадкастинг, по сравнению с 2 минутами 1 секундой со включенным бродкастингом.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_879054177",
      "id": "20211127-025237_1065603256",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:35:13+0300",
      "dateFinished": "2024-02-11T18:35:13+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21849"
    },
    {
      "title": "Вернуть настройку к исходной",
      "text": "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"26214400\")",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:35:25+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1398883795",
      "id": "20201127-230625_1272901030",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:35:25+0300",
      "dateFinished": "2024-02-11T18:35:26+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21850"
    },
    {
      "text": "spark.sql(\"clear cache\")",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:35:33+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = []\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1483817013",
      "id": "20201128-155645_947820002",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:35:33+0300",
      "dateFinished": "2024-02-11T18:35:33+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21851"
    },
    {
      "title": "Задание 4",
      "text": "%md\nВ процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.\n\nДатафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение.",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:35:40+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.</p>\n<p>Датафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1743496551",
      "id": "20201128-163357_1545019956",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:35:40+0300",
      "dateFinished": "2024-02-11T18:35:40+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21852"
    },
    {
      "title": "нужно выполнить, изменять код нельзя",
      "text": "%pyspark \nfrom pyspark.sql.functions import col\n\nskew_df = spark.table(\"hw_4.events_full\")\\\n.where(\"date = '2020-11-01'\")\\\n.repartition(30, col(\"city\"))\\\n.cache()\n\nskew_df.count()",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:35:53+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "10000000\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=1",
              "$$hashKey": "object:24231"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1406560231",
      "id": "20201128-162744_575252973",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:35:53+0300",
      "dateFinished": "2024-02-11T18:36:23+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21853"
    },
    {
      "title": "4.1. Наблюдение проблемы",
      "text": "%md\nПосчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.\n\nВ spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют -- это и является проблемой, которую предлагается решить далее.",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:36:31+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.</p>\n<p>В spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют &ndash; это и является проблемой, которую предлагается решить далее.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1687549024",
      "id": "20201128-164139_1371291032",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:36:31+0300",
      "dateFinished": "2024-02-11T18:36:31+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21854"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import count, col\n\nskew_grouped_df = skew_df \\\n    .groupBy(\"city\").agg(count(\"*\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nskew_grouped_df.show()",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:36:41+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------+-----------+\n|         city|event_count|\n+-------------+-----------+\n|     BIG_CITY|    9000311|\n|SMALL_CITY_51|      10286|\n|SMALL_CITY_19|      10250|\n|SMALL_CITY_48|      10230|\n|SMALL_CITY_81|      10222|\n| SMALL_CITY_4|      10218|\n|SMALL_CITY_38|      10195|\n|SMALL_CITY_98|      10178|\n|SMALL_CITY_85|      10165|\n|SMALL_CITY_62|      10163|\n|SMALL_CITY_57|      10140|\n|SMALL_CITY_22|      10133|\n|SMALL_CITY_79|      10127|\n|SMALL_CITY_69|      10123|\n| SMALL_CITY_5|      10116|\n|SMALL_CITY_70|      10112|\n|SMALL_CITY_63|      10103|\n|SMALL_CITY_64|      10101|\n|SMALL_CITY_47|      10093|\n|SMALL_CITY_32|      10092|\n+-------------+-----------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=2",
              "$$hashKey": "object:24335"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_609780156",
      "id": "20211127-153210_601771328",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:36:41+0300",
      "dateFinished": "2024-02-11T18:36:42+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21855"
    },
    {
      "title": "4.2. repartition",
      "text": "%md\nодин из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num -- количество партиций, на которые будет перемешан исходный датафрейм",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:36:52+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>один из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num &ndash; количество партиций, на которые будет перемешан исходный датафрейм</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_1800643762",
      "id": "20201128-164814_1641460265",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:36:52+0300",
      "dateFinished": "2024-02-11T18:36:52+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21856"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import count, col\n\nskew_grouped_df = skew_df \\\n    .repartition(30) \\\n    .groupBy(\"city\").agg(count(\"*\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nskew_grouped_df.show()\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:37:03+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------+-----------+\n|         city|event_count|\n+-------------+-----------+\n|     BIG_CITY|    9000311|\n|SMALL_CITY_51|      10286|\n|SMALL_CITY_19|      10250|\n|SMALL_CITY_48|      10230|\n|SMALL_CITY_81|      10222|\n| SMALL_CITY_4|      10218|\n|SMALL_CITY_38|      10195|\n|SMALL_CITY_98|      10178|\n|SMALL_CITY_85|      10165|\n|SMALL_CITY_62|      10163|\n|SMALL_CITY_57|      10140|\n|SMALL_CITY_22|      10133|\n|SMALL_CITY_79|      10127|\n|SMALL_CITY_69|      10123|\n| SMALL_CITY_5|      10116|\n|SMALL_CITY_70|      10112|\n|SMALL_CITY_63|      10103|\n|SMALL_CITY_64|      10101|\n|SMALL_CITY_47|      10093|\n|SMALL_CITY_32|      10092|\n+-------------+-----------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=3",
              "$$hashKey": "object:24439"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707578545901_1901789241",
      "id": "paragraph_1707578545901_1901789241",
      "dateCreated": "2024-02-10T18:22:25+0300",
      "dateStarted": "2024-02-11T18:37:03+0300",
      "dateFinished": "2024-02-11T18:37:07+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21857"
    },
    {
      "title": "4.3. Key Salting",
      "text": "%md\nДругой способ исправить неравномерность по ключу -- создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city='BIG_CITY', которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand -- случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n\nТакая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n\nЧто нужно реализовать:\n* добавить синтетический ключ\n* группировка по синтетическому ключу\n* восстановление исходного значения\n* группировка по исходной колонке",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:37:18+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Другой способ исправить неравномерность по ключу &ndash; создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city=&lsquo;BIG_CITY&rsquo;, которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand &ndash; случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным.</p>\n<p>Такая же техника применима и к джойнам по неравномерному ключу, см, например <a href=\"https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\">https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8</a></p>\n<p>Что нужно реализовать:</p>\n<ul>\n<li>добавить синтетический ключ</li>\n<li>группировка по синтетическому ключу</li>\n<li>восстановление исходного значения</li>\n<li>группировка по исходной колонке</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_735024899",
      "id": "20201128-173534_1924644474",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:37:18+0300",
      "dateFinished": "2024-02-11T18:37:18+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21858"
    },
    {
      "text": "%pyspark\n\n# Добавляем колонку с \"солью\": для BIG_CITY - случайное целое от 0 до 20 включительно, для SMALL_CITY - 21, \n# и выводим кусочки датафрейма для BIG_CITY и SMALL_CITY для контроля правильности выполненной процедуры\n\nfrom pyspark.sql.functions import expr, when, sum\n\nsalt = expr(\"\"\"pmod(round((rand() * 100), 0), 20)\"\"\").cast(\"integer\")\n\nsalted_df = skew_df.withColumn(\"salt\", salt) \\\n    .withColumn(\"salt\", when(col(\"city\") == \"BIG_CITY\", col(\"salt\")).otherwise(21)) \n\nsalted_df.show()\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:37:27+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------------+--------------------+----------+----+\n|            event_id|         city|            skew_key|      date|salt|\n+--------------------+-------------+--------------------+----------+----+\n|cef6af8ffb7c923da...|SMALL_CITY_40|cef6af8ffb7c923da...|2020-11-01|  21|\n|e4b7bc4a5c159efa8...|SMALL_CITY_66|e4b7bc4a5c159efa8...|2020-11-01|  21|\n|c83736a98c8a42147...|SMALL_CITY_66|c83736a98c8a42147...|2020-11-01|  21|\n|f3025d662736db1e7...|SMALL_CITY_40|f3025d662736db1e7...|2020-11-01|  21|\n|280ea88a62d880645...|SMALL_CITY_40|280ea88a62d880645...|2020-11-01|  21|\n|f5cab8118b33ffccd...|SMALL_CITY_40|f5cab8118b33ffccd...|2020-11-01|  21|\n|c24c95cd7a7a9f554...|SMALL_CITY_40|c24c95cd7a7a9f554...|2020-11-01|  21|\n|c3e2d396a263a7dbd...|SMALL_CITY_66|c3e2d396a263a7dbd...|2020-11-01|  21|\n|9289bd9093613ac7c...|SMALL_CITY_66|9289bd9093613ac7c...|2020-11-01|  21|\n|0551ae44e789a1251...|SMALL_CITY_40|0551ae44e789a1251...|2020-11-01|  21|\n|b11766352bea40a23...|SMALL_CITY_40|b11766352bea40a23...|2020-11-01|  21|\n|4383f70ed35226bfb...|SMALL_CITY_40|4383f70ed35226bfb...|2020-11-01|  21|\n|0ccdf87c7a444807f...|SMALL_CITY_66|0ccdf87c7a444807f...|2020-11-01|  21|\n|412036a728f75db7b...|SMALL_CITY_40|412036a728f75db7b...|2020-11-01|  21|\n|74758f50f268b2b9b...|SMALL_CITY_40|74758f50f268b2b9b...|2020-11-01|  21|\n|a10d6f963b9bc5a6e...|SMALL_CITY_66|a10d6f963b9bc5a6e...|2020-11-01|  21|\n|4dcd1190a8b4250a3...|SMALL_CITY_40|4dcd1190a8b4250a3...|2020-11-01|  21|\n|a256d42678dfb944f...|SMALL_CITY_66|a256d42678dfb944f...|2020-11-01|  21|\n|eb004109cc6ab1e32...|SMALL_CITY_66|eb004109cc6ab1e32...|2020-11-01|  21|\n|7463bfb40b5b176ad...|SMALL_CITY_40|7463bfb40b5b176ad...|2020-11-01|  21|\n+--------------------+-------------+--------------------+----------+----+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=4",
              "$$hashKey": "object:24543"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_292856067",
      "id": "20211127-172158_1551709606",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:37:27+0300",
      "dateFinished": "2024-02-11T18:37:27+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21859"
    },
    {
      "text": "%pyspark\n\nsalted_count_df = salted_df \\\n    .groupBy(\"city\", \"salt\") \\\n    .count() \n\nresult_df = salted_count_df \\\n    .groupBy(\"city\").agg(sum(\"count\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nresult_df.show()\n",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:40:01+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------+-----------+\n|         city|event_count|\n+-------------+-----------+\n|     BIG_CITY|    9000311|\n|SMALL_CITY_51|      10286|\n|SMALL_CITY_19|      10250|\n|SMALL_CITY_48|      10230|\n|SMALL_CITY_81|      10222|\n| SMALL_CITY_4|      10218|\n|SMALL_CITY_38|      10195|\n|SMALL_CITY_98|      10178|\n|SMALL_CITY_85|      10165|\n|SMALL_CITY_62|      10163|\n|SMALL_CITY_57|      10140|\n|SMALL_CITY_22|      10133|\n|SMALL_CITY_79|      10127|\n|SMALL_CITY_69|      10123|\n| SMALL_CITY_5|      10116|\n|SMALL_CITY_70|      10112|\n|SMALL_CITY_63|      10103|\n|SMALL_CITY_64|      10101|\n|SMALL_CITY_47|      10093|\n|SMALL_CITY_32|      10092|\n+-------------+-----------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-d-02.innoca.local:46770/jobs/job?id=5",
              "$$hashKey": "object:24601"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707472862618_918245763",
      "id": "20211127-172149_80338237",
      "dateCreated": "2024-02-09T13:01:02+0300",
      "dateStarted": "2024-02-11T18:39:07+0300",
      "dateFinished": "2024-02-11T18:39:14+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21860"
    },
    {
      "text": "%pyspark\n\nspark.stop()",
      "user": "anonymous",
      "dateUpdated": "2024-02-11T18:04:55+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1707662270016_50175509",
      "id": "paragraph_1707662270016_50175509",
      "dateCreated": "2024-02-11T17:37:50+0300",
      "dateStarted": "2024-02-11T18:04:55+0300",
      "dateFinished": "2024-02-11T18:04:55+0300",
      "status": "FINISHED",
      "$$hashKey": "object:21861"
    }
  ],
  "name": "homework_4",
  "id": "2JNFCSVPG",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/user_id/homework_4"
}