{
  "metadata": {
    "name": "homework_4",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nspark.executor.instances\u003d2\nspark.executor.memory\u003d1G\nspark.kryoserializer.buffer.max\u003d1024m\n\nspark.sql.autoBroadcastJoinThreshold\u003d20971520"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Нужно скопировать себе эту тетрадку. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.mllib.random.RandomRDDs._\nimport java.time.LocalDate\nimport java.time.format.DateTimeFormatter\n\nval dates \u003d (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n\ndef generateCity(r: Double): String \u003d if (r \u003c 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n\ndef generateCityUdf \u003d udf(generateCity _)\n\n// spark.sql(\"drop table hw2.events_full\")\nspark.sql(\"create database hw_4\")\nfor(i \u003c- dates) {\n    uniformRDD(sc, 10000000L, 1)\n    .toDF(\"uid\")\n    .withColumn(\"date\", lit(i))\n    .withColumn(\"city\", generateCityUdf($\"uid\"))\n    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n    .withColumn(\"skew_key\", when($\"city\" \u003d\u003d\u003d \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n    .write.mode(\"append\")\n    .partitionBy(\"date\")\n    .saveAsTable(\"hw_4.events_full\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.001)\n.repartition(2)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nspark.table(\"hw_4.sample\")\n.limit(100)\n.coalesce(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_small\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.003)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_big\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.015)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_very_big\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Для упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n\nОтветить на вопросы:\n * какова структура таблиц\n * сколько в них записей \n * сколько места занимают данные\n "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col\n\ntables \u003d spark.sql(\"SHOW TABLES IN hw_4\").collect()\n\nfor table in tables:\n    \n    # Имя объекта\n    print(f\"{table[\u0027database\u0027]}.{table[\u0027tableName\u0027]}\")\n    \n    # Статистика\n    compute_df \u003d spark.sql(f\"ANALYZE TABLE {table[\u0027database\u0027]}.{table[\u0027tableName\u0027]} COMPUTE STATISTICS\")\n    describe_df \u003d spark.sql(f\"DESCRIBE FORMATTED {table[\u0027database\u0027]}.{table[\u0027tableName\u0027]}\")\n    statistics \u003d describe_df \\\n        .where(col(\"col_name\") \u003d\u003d \"Statistics\") \\\n        .select(\"data_type\") \\\n        .rdd.map(lambda x: x[0]) \\\n        .collect()\n    print(statistics[0])    \n    \n    # Схема таблицы\n    spark.table(f\"{table[\u0027database\u0027]}.{table[\u0027tableName\u0027]}\").printSchema()\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Информация о том, сколько места занимают данные посмотреть в HDFS UI"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Получить планы запросов для джойна большой таблицы hw_4.events_full с каждой из таблиц hw_4.sample, hw_4.sample_big, hw_4.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? \n\nBroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nconf \u003d spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nprint(conf)\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\n\n# BroadcastHashJoin\nsample_small_df \u003d spark.table(\"hw_4.sample_small\")\nef_sample_small_df \u003d events_full_df \\\n    .join(sample_small_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# BroadcastHashJoin\nsample_df \u003d spark.table(\"hw_4.sample\")\nef_sample_df \u003d events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# SortMergeJoin\nsample_big_df \u003d spark.table(\"hw_4.sample_big\")\nef_sample_big_df \u003d events_full_df \\\n    .join(sample_big_df, \"event_id\", \"inner\") \\\n    .explain()\n\n# SortMergeJoin\nsample_very_big_df \u003d spark.table(\"hw_4.sample_very_big\")\nef_sample_very_big_df \u003d events_full_df \\\n    .join(sample_very_big_df, \"event_id\", \"inner\") \\\n    .explain()\n\n#ef_sample_df.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Выполнить джойны с таблицами  hw_4.sample,  hw_4.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении\n\nЗайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  "
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 84 tasks, \n# время выполнения 1 мин 8 сек, \n# количество task меньше, чем в операции ниже за счет наличия broadcastб \n# в даге 2 стейджа, реализующие механизм broadcast - передача малого sample датафрема на worker\u0027ы с их последющим джойном к \"большой\" таблице event_full\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample\")\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\nsample_df \u003d spark.table(\"hw_4.sample\")\n\nef_sample_df \u003d events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 284 tasks, время выполнения 2 мин 38 сек,\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample_big\")\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\nsample_big_df \u003d spark.table(\"hw_4.sample_big\")\n\nef_sample_big_df \u003d events_full_df \\\n    .join(sample_big_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Оптимизировать джойн с таблицами hw_4.sample_big, hw_4.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится "
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import broadcast\n\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample_big_broad\")\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\nsample_big_df \u003d spark.table(\"hw_4.sample_big\")\nsample_big_df_broad \u003d broadcast(sample_big_df)\n\nef_sample_big_df \u003d events_full_df \\\n    .join(sample_big_df_broad, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import broadcast\n\nsc.setLocalProperty(\"callSite.short\", \"events_full br join sample_big\")\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\nsample_very_big_df \u003d spark.table(\"hw_4.sample_very_big\")\nsample_very_big_df_broad \u003d broadcast(sample_very_big_df)\n\nef_sample_big_df \u003d events_full_df \\\n    .join(sample_very_big_df_broad, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_big_df)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Таблица hw_4.sample_very_big оказывается слишком большой для бродкаста и не помещается полностью на каждой ноде, поэтому возникает исключение.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Отключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nsc.setLocalProperty(\"callSite.short\", \"events_full join sample without broadcast\")\n\nevents_full_df \u003d spark.table(\"hw_4.events_full\")\nsample_df \u003d spark.table(\"hw_4.sample\")\n\nef_sample_df \u003d events_full_df \\\n    .join(sample_df, \"event_id\", \"inner\") \\\n    .count()\n\nprint(ef_sample_df)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "6 минут 2 секунды в случае, когда отключен автоматический броадкастинг, по сравнению с 2 минутами 1 секундой со включенным бродкастингом.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"26214400\")"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.sql(\"clear cache\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.\n\nДатафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \nfrom pyspark.sql.functions import col\n\nskew_df \u003d spark.table(\"hw_4.events_full\")\\\n.where(\"date \u003d \u00272020-11-01\u0027\")\\\n.repartition(30, col(\"city\"))\\\n.cache()\n\nskew_df.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.\n\nВ spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют -- это и является проблемой, которую предлагается решить далее."
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import count, col\n\nskew_grouped_df \u003d skew_df \\\n    .groupBy(\"city\").agg(count(\"*\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nskew_grouped_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "один из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num -- количество партиций, на которые будет перемешан исходный датафрейм"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import count, col\n\nskew_grouped_df \u003d skew_df \\\n    .repartition(30) \\\n    .groupBy(\"city\").agg(count(\"*\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nskew_grouped_df.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Другой способ исправить неравномерность по ключу -- создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city\u003d\u0027BIG_CITY\u0027, которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand -- случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n\nТакая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n\nЧто нужно реализовать:\n* добавить синтетический ключ\n* группировка по синтетическому ключу\n* восстановление исходного значения\n* группировка по исходной колонке"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Добавляем колонку с \"солью\": для BIG_CITY - случайное целое от 0 до 20 включительно, для SMALL_CITY - 21, \n# и выводим кусочки датафрейма для BIG_CITY и SMALL_CITY для контроля правильности выполненной процедуры\n\nfrom pyspark.sql.functions import expr, when, sum\n\nsalt \u003d expr(\"\"\"pmod(round((rand() * 100), 0), 20)\"\"\").cast(\"integer\")\n\nsalted_df \u003d skew_df.withColumn(\"salt\", salt) \\\n    .withColumn(\"salt\", when(col(\"city\") \u003d\u003d \"BIG_CITY\", col(\"salt\")).otherwise(21)) \n\nsalted_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsalted_count_df \u003d salted_df \\\n    .groupBy(\"city\", \"salt\") \\\n    .count() \n\nresult_df \u003d salted_count_df \\\n    .groupBy(\"city\").agg(sum(\"count\").alias(\"event_count\")) \\\n    .orderBy(col(\"event_count\").desc())\n    \nresult_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.stop()"
    }
  ]
}